{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitlifelongenv61ea68d4443441b5a1badc5d8f0e38f5",
   "display_name": "Python 3.8.5 64-bit ('lifelong_env')",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorlayer as tl\n",
    "from tensorlayer import logging\n",
    "from tensorlayer.decorators import deprecated_alias\n",
    "from tensorlayer.layers.core import Layer\n",
    "from tensorlayer.layers.utils import get_collection_trainable\n",
    "\n",
    "from tensorlayer.layers import *\n",
    "from tensorlayer.models import Model\n",
    "\n",
    "lrelu = lambda x: tl.act.lrelu(x, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeConv1d(Layer):\n",
    "    \"\"\"Simplified version of :class:`DeConv1dLayer`, see `tf.nn.conv1d_transpose <https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/nn/conv1d_transpose>`__.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_filter : int\n",
    "        The number of filters.\n",
    "    filter_size : tuple of int\n",
    "        The filter size width.\n",
    "    strides : tuple of int\n",
    "        The stride step width.\n",
    "    padding : str\n",
    "        The padding algorithm type: \"SAME\" or \"VALID\".\n",
    "    act : activation function\n",
    "        The activation function of this layer.\n",
    "    data_format : str\n",
    "        \"channels_last\" (NHWC, default) or \"channels_first\" (NCHW).\n",
    "    dilation_rate : int of tuple of int\n",
    "        The dilation rate to use for dilated convolution\n",
    "    W_init : initializer\n",
    "        The initializer for the weight matrix.\n",
    "    b_init : initializer or None\n",
    "        The initializer for the bias vector. If None, skip biases.\n",
    "    in_channels : int\n",
    "        The number of in channels.\n",
    "    name : None or str\n",
    "        A unique layer name.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    With TensorLayer\n",
    "\n",
    "    >>> net = tl.layers.Input([5, 100, 100, 32], name='input')\n",
    "    >>> deconv2d = tl.layers.DeConv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), in_channels=32, name='DeConv2d_1')\n",
    "    >>> print(deconv2d)\n",
    "    >>> tensor = tl.layers.DeConv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), name='DeConv2d_2')(net)\n",
    "    >>> print(tensor)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_filter=32,\n",
    "        filter_size=3,\n",
    "        strides=2,\n",
    "        act=None,\n",
    "        padding='SAME',\n",
    "        dilation_rate=1,\n",
    "        data_format='channels_last',\n",
    "        W_init=tl.initializers.truncated_normal(stddev=0.02),\n",
    "        b_init=tl.initializers.constant(value=0.0),\n",
    "        in_channels=None,\n",
    "        name=None  # 'decnn2d'\n",
    "    ):\n",
    "        super().__init__(name, act=act)\n",
    "        self.n_filter = n_filter\n",
    "        self.filter_size = filter_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.data_format = data_format\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.W_init = W_init\n",
    "        self.b_init = b_init\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # Attention: To build, we need not only the in_channels!\n",
    "        # if self.in_channels:\n",
    "        #     self.build(None)\n",
    "        #     self._built = True\n",
    "\n",
    "        logging.info(\n",
    "            \"DeConv1d {}: n_filters: {} strides: {} padding: {} act: {} dilation: {}\".format(\n",
    "                self.name,\n",
    "                str(n_filter),\n",
    "                str(strides),\n",
    "                padding,\n",
    "                self.act.__name__ if self.act is not None else 'No Activation',\n",
    "                dilation_rate,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if type(strides) != int:\n",
    "            raise ValueError(\"type(strides) should be int... Like in tensorflow\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        actstr = self.act.__name__ if self.act is not None else 'No Activation'\n",
    "        s = (\n",
    "            '{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}'\n",
    "            ', strides={strides}, padding={padding}'\n",
    "        )\n",
    "        if self.dilation_rate != 1:\n",
    "            s += ', dilation={dilation_rate}'\n",
    "        if self.b_init is None:\n",
    "            s += ', bias=False'\n",
    "        s += (', ' + actstr)\n",
    "        if self.name is not None:\n",
    "            s += ', name=\\'{name}\\''\n",
    "        s += ')'\n",
    "        return s.format(classname=self.__class__.__name__, **self.__dict__)\n",
    "\n",
    "    def build(self, inputs_shape):\n",
    "        self.layer = tf.keras.layers.Conv1DTranspose(\n",
    "            filters=self.n_filter,\n",
    "            kernel_size=self.filter_size,\n",
    "            strides=self.strides,\n",
    "            padding=self.padding,\n",
    "            data_format=self.data_format,\n",
    "            dilation_rate=self.dilation_rate,\n",
    "            activation=self.act,\n",
    "            use_bias=(True if self.b_init is not None else False),\n",
    "            kernel_initializer=self.W_init,\n",
    "            bias_initializer=self.b_init,\n",
    "            # dtype=tf.float32,\n",
    "            name=self.name,\n",
    "        )\n",
    "        if self.data_format == \"channels_first\":\n",
    "            self.in_channels = inputs_shape[1]\n",
    "        else:\n",
    "            self.in_channels = inputs_shape[-1]\n",
    "        _out = self.layer(\n",
    "            tf.convert_to_tensor(np.random.uniform(size=inputs_shape), dtype=np.float32)\n",
    "        )  #np.random.uniform([1] + list(inputs_shape)))  # initialize weights\n",
    "        outputs_shape = _out.shape\n",
    "        self._trainable_weights = self.layer.weights\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.layer(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[TL] DeConv1d testeG_conv_2: n_filters: 55 strides: 2 padding: SAME act: No Activation dilation: 1\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DeConv1d(in_channels=None, out_channels=55, kernel_size=4, strides=2, padding=SAME, bias=False, No Activation, name='testeG_conv_2')"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "w_init = tl.initializers.truncated_normal(stddev=0.01)\n",
    "prefix = 'teste'\n",
    "i = 0\n",
    "DeConv1d(55, 4, 2, W_init=w_init, b_init=None, name=prefix+'G_conv_{}'.format(i + 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(R, n_f, f_s):\n",
    "    w_init = tl.initializers.truncated_normal(stddev=0.01)\n",
    "    R_tmp = R\n",
    "    R = BatchNorm1d(act=tf.nn.relu)(Conv1d(n_f, f_s, 1, W_init=w_init)(R))\n",
    "    R = BatchNorm1d(act=None)(Conv1d(n_f, f_s, 1, W_init=w_init)(R))\n",
    "    R_tmp = Conv1d(n_f, 1, 1)(R_tmp)\n",
    "    return Elementwise(tf.add, act=tf.nn.relu)([R_tmp, R])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator(input_shape, prefix = \"\"):\n",
    "\tI = Input(input_shape)\n",
    "\tD = Conv1d(\n",
    "\t\t64, 3, 2, padding='SAME', act=lrelu, b_init=None, name=prefix+'D_conv_1')(I)\n",
    "\tD = InstanceNorm1d(act=lrelu)(Conv1d(\n",
    "\t\t128, 3, 2, padding='SAME', b_init=None, name=prefix+'D_conv_2')(D))\n",
    "\tD = InstanceNorm1d(act=lrelu)(Conv1d(\n",
    "\t\t256, 3, 2, padding='SAME', b_init=None, name=prefix+'D_conv_3')(D))\n",
    "\tD = InstanceNorm1d(act=lrelu)(Conv1d(\n",
    "\t\t512, 3, 2, padding='SAME', b_init=None, name=prefix+'D_conv_4')(D))\n",
    "\tD = InstanceNorm1d(act=lrelu)(Conv1d(\n",
    "\t\t512, 3, 2, padding='SAME', b_init=None, name=prefix+'D_conv_5')(D))\n",
    "\tD = InstanceNorm1d(act=lrelu)(Conv1d(\n",
    "\t\t512, 3, 2, padding='SAME', b_init=None, name=prefix+'D_conv_6')(D))\n",
    "\tD = Conv1d(1, 3, 1, name=prefix+'D_conv_7')(D)\n",
    "\tD = GlobalMeanPool1d()(D)\n",
    "\tD_net = Model(inputs=I, outputs=D, name=prefix+'Discriminator')\n",
    "\treturn D_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(input_shape, z_dim, prefix = \"\"):\n",
    "    w_init = tl.initializers.truncated_normal(stddev=0.01)\n",
    "    I = Input(input_shape)\n",
    "    Z = Input([input_shape[0], z_dim])\n",
    "    z = Reshape((input_shape[0], 1, 1, -1))(Z)\n",
    "    print('AQUI')\n",
    "    print(z)\n",
    "    z = Tile([1, input_shape[1], 1])(z)\n",
    "    print(z.shape)\n",
    "    conv_layers = []\n",
    "    G = Concat(concat_dim=-1)([I, z])\n",
    "    filters = [64, 128, 256, 512, 512, 512, 512]\n",
    "    if image_size == 256:\n",
    "    \tfilters.append(512)\n",
    "    G = Conv2d(\n",
    "    \tfilters[0], (4, 4), (2, 2), act=lrelu, W_init=w_init, b_init=None, name=prefix+'G_conv_1')(G)\n",
    "    conv_layers.append(G)\n",
    "    for i, n_f in enumerate(filters[1:]):\n",
    "    \tG = BatchNorm2d(act=lrelu)(Conv2d(\n",
    "    \t\tn_f, (4, 4), (2, 2), W_init=w_init, b_init=None, name=prefix+'G_conv_{}'.format(i + 2))(G))\n",
    "    \tconv_layers.append(G)   \n",
    "    filters.pop()\n",
    "    filters.reverse()\n",
    "    conv_layers.pop()\n",
    "    for i, n_f in enumerate(filters):\n",
    "    \tG = BatchNorm2d(act=tf.nn.relu)(DeConv2d(\n",
    "    \t\tn_f, (4, 4), (2, 2), W_init=w_init, b_init=None, name=prefix+'G_deconv_{}'.format(len(filters)+1-i))(G))\n",
    "    \tG = Concat(concat_dim=-1)([G, conv_layers.pop()])\n",
    "    G = DeConv2d(3, (4, 4), (2, 2), act=tf.nn.tanh, W_init=w_init, b_init=None, name=prefix+'G_deconv_1')(G)\n",
    "    G_net = Model(inputs=[I, Z], outputs=G, name=prefix+'Generator')\n",
    "    return G_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[TL] Input  _inputlayer_5: (4, 100)\n",
      "[TL] Input  _inputlayer_6: [4, 100]\n",
      "[TL] Reshape reshape_3\n",
      "AQUI\n",
      "tf.Tensor(\n",
      "[[[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]]], shape=(4, 1, 1, 100), dtype=float32)\n",
      "[TL] Tile  tile_2: multiples: [1, 100, 1]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "InvalidArgumentError",
     "evalue": "Expected multiples argument to be a vector of length 4 but got length 3 [Op:Tile]",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-5997c5bbcd2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-16573b33aca3>\u001b[0m in \u001b[0;36mGenerator\u001b[0;34m(input_shape, z_dim, prefix)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AQUI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mconv_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/lifelong_ringer/lifelong_env/lib/python3.8/site-packages/tensorlayer/layers/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_built\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_fixed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/lifelong_ringer/lifelong_env/lib/python3.8/site-packages/tensorlayer/layers/extend.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# @tf.function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/lifelong_ringer/lifelong_env/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtile\u001b[0;34m(input, multiples, name)\u001b[0m\n\u001b[1;32m  11452\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11454\u001b[0;31m       return tile_eager_fallback(\n\u001b[0m\u001b[1;32m  11455\u001b[0m           input, multiples, name=name, ctx=_ctx)\n\u001b[1;32m  11456\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/lifelong_ringer/lifelong_env/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtile_eager_fallback\u001b[0;34m(input, multiples, name, ctx)\u001b[0m\n\u001b[1;32m  11492\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11493\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tmultiples\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_Tmultiples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11494\u001b[0;31m   _result = _execute.execute(b\"Tile\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0m\u001b[1;32m  11495\u001b[0m                              ctx=ctx, name=name)\n\u001b[1;32m  11496\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/lifelong_ringer/lifelong_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected multiples argument to be a vector of length 4 but got length 3 [Op:Tile]"
     ]
    }
   ],
   "source": [
    "Generator((4, 100), z_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}